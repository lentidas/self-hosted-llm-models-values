tgi:
  model: meta-llama/Meta-Llama-3.1-8B-Instruct

  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      cpu: 2
      memory: 5Gi

  tolerations:
  - key: nodepool
    operator: Equal
    value: llm
    effect: NoSchedule

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: role
            operator: In
            values:
            - llm

  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
